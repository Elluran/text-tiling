{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from nltk.metrics import windowdiff\n",
    "from utils import *\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSPDataset(Dataset):\n",
    "    def __init__(self, message_pairs, device=torch.device(\"cuda:0\")):\n",
    "        self.message_pairs = message_pairs\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.device = device\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.message_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        message_1 = self.message_pairs[idx][0][0]\n",
    "        message_2 = self.message_pairs[idx][0][1]\n",
    "\n",
    "        tokenized_input = self.tokenizer(\n",
    "            message_1,\n",
    "            message_2,\n",
    "            max_length=25,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        label = torch.tensor(self.message_pairs[idx][1])\n",
    "\n",
    "        tokenized_input[\"input_ids\"] = tokenized_input[\"input_ids\"][0]\n",
    "        tokenized_input[\"token_type_ids\"] = tokenized_input[\"token_type_ids\"][0]\n",
    "        tokenized_input[\"attention_mask\"] = tokenized_input[\"attention_mask\"][0]\n",
    "\n",
    "        return tokenized_input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_message_pairs, test_message_pairs = train_test_split(list(zip(message_pairs, labels)), random_state=42)\n",
    "\n",
    "train_dataset = NSPDataset(train_message_pairs)\n",
    "test_dataset = NSPDataset(test_message_pairs)\n",
    "quick_test_dataset = NSPDataset(test_message_pairs[:20])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=80, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=80)\n",
    "quick_test_dataloader = torch.utils.data.DataLoader(quick_test_dataset, batch_size=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for i in test_dataset:\n",
    "    s += sum(i[0].input_ids > 0)\n",
    "s / 25 / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, scheduler, dataloader, device=torch.device(\"cuda:0\")):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    for num, (inp, target) in tqdm(enumerate(dataloader)):\n",
    "        inp.to(device)\n",
    "        output = model(**inp)\n",
    "\n",
    "        loss = loss_fn(output.logits, target.to(device))\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if num % 100 == 0:\n",
    "            validate(model, quick_test_dataloader)\n",
    "\n",
    "    print(np.mean(losses))\n",
    "\n",
    "def validate(mode, dataloader, device=torch.device(\"cuda:0\")):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for inp, target in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            inp.to(device)\n",
    "            output = model(**inp)\n",
    "            loss = loss_fn(output.logits, target.to(device))\n",
    "            losses.append(loss.item())\n",
    "            # print(loss)\n",
    "    model.train()\n",
    "    print(np.mean(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# model.cpu()\n",
    "# del quick_test_dataloader\n",
    "# del train_dataloader\n",
    "# del test_dataloader\n",
    "# del model \n",
    "# del optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForNextSentencePrediction.from_pretrained(\"distilbert-base-uncased\").to(\"cuda:0\")\n",
    "lr = 1e-5\n",
    "model = BertForNextSentencePrediction.from_pretrained(\"prajjwal1/bert-medium\").to(\"cuda:0\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr , steps_per_epoch=len(train_dataset), epochs=epochs, anneal_strategy='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    train(model, optimizer, scheduler, train_dataloader)\n",
    "    validate(model, test_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
